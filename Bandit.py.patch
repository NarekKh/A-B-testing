# Patch generated by Pyment v0.3.3

--- a/Bandit.py
+++ b/Bandit.py
@@ -10,6 +10,7 @@
 
 
 class CustomFormatter(logging.Formatter):
+    """ """
     # Defining custom log formatting
     grey = "\x1b[38;20m"
     yellow = "\x1b[33;20m"
@@ -26,6 +27,12 @@
     }
 
     def format(self, record):
+        """
+        
+
+        :param record: 
+
+        """
         log_fmt = self.FORMATS.get(record.levelno)
         formatter = logging.Formatter(log_fmt)
         return formatter.format(record)
@@ -37,6 +44,7 @@
 
 
 class Bandit(ABC):
+    """ """
     @abstractmethod
     def __init__(self, p):
         # Initializing the bandit with a probability distribution p
@@ -50,13 +58,27 @@
 
     @abstractmethod
     def pull(self):
+        """ """
         pass
 
     @abstractmethod
     def update(self, arm, reward):
+        """
+        
+
+        :param arm: 
+        :param reward: 
+
+        """
         pass
 
     def experiment(self, num_trials):
+        """
+        
+
+        :param num_trials: 
+
+        """
         # Running an experiment with num_trials iterations
         for _ in range(num_trials):
             arm = self.pull()
@@ -67,6 +89,12 @@
             self.update(arm, reward)  # Updating bandit's state
 
     def report(self, algorithm):
+        """
+        
+
+        :param algorithm: 
+
+        """
         # Reporting results of the experiment
         with open(f'{algorithm}_results.csv', mode='w', newline='') as file:
             writer = csv.writer(file)
@@ -80,6 +108,7 @@
 
 
 class EpsilonGreedy(Bandit):
+    """ """
     def __init__(self, p, initial_epsilon):
         super().__init__(p)
         self.epsilon = initial_epsilon  # Initializing epsilon
@@ -90,18 +119,27 @@
         return 'EpsilonGreedy'
 
     def pull(self):
+        """ """
         if random.random() < self.epsilon:
             return random.randint(0, len(self.p) - 1)
         else:
             return self.q_values.index(max(self.q_values))
 
     def update(self, arm, reward):
+        """
+        
+
+        :param arm: 
+        :param reward: 
+
+        """
         self.action_counts[arm] += 1  # Incrementing action count
         self.q_values[arm] += (reward - self.q_values[arm]) / self.action_counts[arm]  # Updating q-value
         self.epsilon = 1 / (sum(self.action_counts) + 1)  # Decay epsilon
 
 
 class ThompsonSampling(Bandit):
+    """ """
     def __init__(self, p, precision):
         super().__init__(p)
         self.precision = precision  # Setting precision
@@ -112,10 +150,18 @@
         return 'ThompsonSampling'
 
     def pull(self):
+        """ """
         samples = [random.betavariate(self.alpha[i], self.beta[i]) for i in range(len(self.p))]
         return samples.index(max(samples))
 
     def update(self, arm, reward):
+        """
+        
+
+        :param arm: 
+        :param reward: 
+
+        """
         self.alpha[arm] += reward
         self.beta[arm] += (1 - reward)
         if self.alpha[arm] <= 0 or self.beta[arm] <= 0:
@@ -125,8 +171,16 @@
 
 
 class Visualization:
+    """ """
     @staticmethod
     def plot1(epsilon_greedy_rewards, thompson_rewards):
+        """
+        
+
+        :param epsilon_greedy_rewards: 
+        :param thompson_rewards: 
+
+        """
         import matplotlib.pyplot as plt
 
         plt.plot(epsilon_greedy_rewards, label='Epsilon Greedy')
@@ -139,6 +193,13 @@
 
     @staticmethod
     def plot2(e_greedy_rewards, thompson_rewards):
+        """
+        
+
+        :param e_greedy_rewards: 
+        :param thompson_rewards: 
+
+        """
         import matplotlib.pyplot as plt
 
         cumulative_e_greedy_rewards = [sum(e_greedy_rewards[:i + 1]) for i in range(len(e_greedy_rewards))]
@@ -154,6 +215,7 @@
 
 
 def comparison():
+    """ """
     pass
 
 # Running
